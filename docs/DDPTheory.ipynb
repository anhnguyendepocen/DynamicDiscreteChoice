{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Discrete Choice Models\n",
    "\n",
    "In a dynamic discrete choice (DDC) model an agent chooses a series of actions according to its preferences, cognitive biases and beliefs about the future.\n",
    "\n",
    "A common assumption for DDC models is the random utility assumption. Under the random utility assusmption, an agent chooses the action that maximizes their present discounted utility (in other words they have rational expectations about the future and their choices are free from cognitive bias). This means that the agent being analyzed by the researcher is behaving according to a Markov Decision Process.\n",
    "\n",
    "In a Markov Decision Process (MDP), an agent picks a policy (a function mapping states to actions) that maximizes their present discounted utility. A MDP is a tuple, $(S, A, T, U, \\beta)$, where\n",
    "\n",
    "- $S$ is the state space\n",
    "- $A$ is the action space\n",
    "- $T$ is a transition distribution (a function that takes a state and an action returns a probability distribution of states for the next period)\n",
    "- $U$ is function that maps state action pairs to utils\n",
    "- $\\beta$ is the discount rate\n",
    "\n",
    "DDC models typically partition the state into two pieces: those observed by the researcher and those that are not.\n",
    "\n",
    "- $x$: tuple of agent state variables observed by the researcher\n",
    "- $\\epsilon$: tuple of agent state variables unobserved by the researcher\n",
    "\n",
    "DDC models also make a number of assumptions about the structure of the MDP in order to make estimation simple. Some common assumptions are:\n",
    "\n",
    "- Additive seperability: The utility function is  given by $\\mu(s, a) = u(x, a) + \\epsilon$\n",
    "- Conditional independence: The transition function can be decomposed into $T(s' | s, a) = P_x(x' | x, a)P_\\epsilon(\\epsilon' | \\epsilon, a)$\n",
    "- IID unobserved state variable: $\\epsilon$ is IID\n",
    "- Conditional Logit: $\\epsilon$ is Gumbel distributed\n",
    "\n",
    "One of the first methods to estimate the parameters of the utility function in a DDC given a series of state and action observations is the NFXP method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFXP\n",
    "\n",
    "The NFXP uses the Additive seperability, conditional independence, IID unobserved state variable and conditional logit assumptions to greatly simplify estimation. An outline of how the assumptions simplify the value function is shown below. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V(x,\\epsilon)&=\\max_{d\\in D}\\mu(x,d,\\epsilon)+\\beta E_{x',\\epsilon'}[V(x',\\epsilon'|x,\\epsilon)]\\\\\n",
    "    &=\\max_{d\\in D}u(x,d)+\\epsilon+\\beta E_{x',\\epsilon'}[V(x',\\epsilon'|x,\\epsilon)]\\quad\\text{AS}\\\\\n",
    "    &=\\max_{d\\in D}u(x,d)+\\epsilon+\\beta\\int_{x'}\\int_{\\epsilon'}V(x',\\epsilon')p(x',\\epsilon'|x,\\epsilon,d)d\\epsilon'dx'\\\\\n",
    "    &=\\max_{d\\in D}u(x,d)+\\epsilon+\\beta\\int_{x'}\\int_{\\epsilon'}V(x',\\epsilon')p(\\epsilon'|\\epsilon,d)q(x'|x,d)d\\epsilon'dx'\\quad\\text{CI}\\\\\n",
    "    &=\\max_{d\\in D}u(x,d)+\\epsilon+\\beta\\int_{x'}\\int_{\\epsilon'}V(x',\\epsilon')p(\\epsilon_{1})\\cdots p(\\epsilon_{|\\epsilon|})d\\epsilon'q(x'|x,d)dx'\\quad\\text{IID}\\\\\n",
    "    &=\\max_{d\\in D}u(x,d)+\\epsilon+\\beta\\int_{x'}\\overline{V(x')}q(x'|x,d)dx'\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When the observed state variables, x, are discrete the integral is replaced with a summation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\overline{V(x)}&=\n",
    "\t\t\\int_{\\epsilon}\\left(\\max_{d\\in D} u(x,d) + \\epsilon(d) + \n",
    "\t\t\\beta\\sum_{x'}\\overline{V(x')} q(x' | x, d)\\right)dP(\\epsilon) \\notag\\\\\n",
    "\t&=\\log\\left[\\sum_{d=0}^{D}\\exp\\left\\{u(x,d)+\\beta\\sum_{x'}\\overline{V(x')}q(x'|x,d)\\right\\} \t\t\\right]\\quad\\text{Gumbel Distribution}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The probability of choosing an action given the states observed by the researcher is\n",
    "\n",
    "$$\n",
    "P(d|x_{t},\\theta)=\\frac{\\exp\\{v(d,x_{t}\\}}{\\sum_{j=0}^{J}\\exp\\{v(j,x_{t})\\}}\n",
    "$$ \n",
    "\n",
    "This bears a strong resemblance to the choice proability in logistic regression. The likelihood is simply the probability density of the first observation times the transition probabilities for the whole series times the choice probabilities. Logged this gives   \n",
    "\n",
    "$$\n",
    "l_{i}(\\theta)=\\sum_{t=1}^{T}\\log P(d=d_{it}|x_{it},\\theta)+\\left(\\sum_{t=1}^{T-1}\\log q(x_{it+1}|x_{it},d_{it},\\theta)\\right)+\\log Pr(x_{i1}|\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Interfaces\n",
    "\n",
    "### Dynamic Discrete Choice\n",
    "\n",
    "| Method                    | Description             |\n",
    "|:---------------           |:------------------------|\n",
    "|`fit(ddcfamily, data, algorithm)`| Fit the dynamic discrete choice model on some data using an algorithm |\n",
    "\n",
    "\n",
    "### Nested Fixed Point\n",
    "\n",
    "| Method                           | Description             |\n",
    "|:---------------                  |:------------------------|\n",
    "|`stage1_problem(nfxpfamily, data)`| Get the problem description for stage 1. This description is used by a solver to find the optimal transition parameters |\n",
    "|`stage2_problem(nfxpfamily, data)`| Get the problem description for stage 2. This description is used by a solver to find the optimal utility parameters |\n",
    "\n",
    "### MDP Result\n",
    "\n",
    "| Method                      | Description             |\n",
    "|:---------------             |-----------------------:|\n",
    "|`policy(ddcresult, state)`   | Return the policy at the current state |\n",
    "|`sample(ddcresult, state, n)`| Generate `n` samples from the MDP, using `state` as the starting state |\n",
    "\n",
    "### Discrete Transition\n",
    "\n",
    "| Method                                                 | Description                                     |\n",
    "|:---------------------------------------------          |:------------------------------------------------|\n",
    "|`condprob(transition, state_ind, action_ind)`           | Return the conditional dist. of a transition    |\n",
    "|`prob(transition, state_ind, action_ind, new_state_ind)`| Return the probability of a transition occuring |\n",
    "\n",
    "Both `condprob` and `prob` should return a vector of probabilities.\n",
    "\n",
    "### Discrete VariableSpaces\n",
    "\n",
    "| Method                | Description             |\n",
    "|:---------------       |:------------------------|\n",
    "|`levels(vs)`           | Return an iterable of all possible levels |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators\n",
    "\n",
    "- Nested Fixed Point (MPEC)\n",
    "- Nested Pseudo Likelihood\n",
    "- Mathematical Program with Equilibrium Constraints (MPEC)\n",
    "- Expectation Maximization\n",
    "\n",
    "# Confidence Interval\n",
    "\n",
    "- Parametric Bootstrap\n",
    "- Resampling Bootstrap\n",
    "- BHHH\n",
    "\n",
    "# Statistics\n",
    "\n",
    "- cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "[@SuJudd1012]: http://people.hss.caltech.edu/~mshum/gradio/papers/SuJudd2012.pdf \"Constrained Optimzation Approaches to Estimation of Structural Models\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0-pre",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
